\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Math 656 - HW3},
            pdfauthor={Jeff Gould},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}

\title{Math 656 - HW3}
\author{Jeff Gould}
\date{9/15/2020}

\begin{document}
\maketitle

\hypertarget{exercise-3}{%
\subsubsection{Exercise 3}\label{exercise-3}}

\hypertarget{a-what-is-the-entropy-of-this-collection-of-training-examples-with-respect-to-the-class-attribute}{%
\paragraph{a) What is the entropy of this collection of training
examples with respect to the class
attribute?}\label{a-what-is-the-entropy-of-this-collection-of-training-examples-with-respect-to-the-class-attribute}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Table3}\FloatTok{.6}\NormalTok{ <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{Instance =} \KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{1}\NormalTok{),}
  \DataTypeTok{a1 =} \KeywordTok{c}\NormalTok{(T, T, T, F, F, F, F, T, F),}
  \DataTypeTok{a2 =} \KeywordTok{c}\NormalTok{(T, T, F, F, T, T, F, F, T),}
  \DataTypeTok{a3 =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{),}
  \DataTypeTok{TargetClass =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Entropy is defined as \(H(N) = -\sum P(i|N)\log_2(P(i|N))\). We have two
classes, + (denoted with 1's) and -, denoted with 0's.

There are 4 instances of +, and 5 instances of -, and \(N=9\). So the
entropy is given by:

\(H(N) = -\sum_{i=1}^9 P(i|9)\log_2 (P(i|9)) = -P(+|N)\log_2(P(+|N))-P(-|N)\log_2(P(-|N)) = -\frac{4}{9}\log_2(\frac{4}{9})-\frac{5}{9}\log_2(\frac{5}{9}) =\)
0.9910761

\hypertarget{b-what-are-the-information-gains-of-a_1-and-a_2-relative-to-these-training-examples}{%
\paragraph{\texorpdfstring{b) What are the information gains of \(a_1\)
and \(a_2\) relative to these training
examples?}{b) What are the information gains of a\_1 and a\_2 relative to these training examples?}}\label{b-what-are-the-information-gains-of-a_1-and-a_2-relative-to-these-training-examples}}

Information gain at a node is equal to the entropy of the class
attribute minue the entropy at the node, ie at node \(a_1\) the
information gain is \(H(N)-H(a_1)\)

\(H(a_1) = P(a_1 = F)H(a_1=F)+P(a_1=T)H(a_1=T)\)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ftable}\NormalTok{(Table3}\FloatTok{.6}\NormalTok{[,}\KeywordTok{c}\NormalTok{(}\StringTok{"a1"}\NormalTok{, }\StringTok{"TargetClass"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\(H(a_1 = F) = -\frac{4}{5}\log_2\left(\frac{4}{5}\right)-\frac{1}{5}\log_2\left(\frac{1}{5}\right)\)

\(P(a_1 = F) = 5/9\)

\(H(a_1 = T) = -\frac{3}{4}\log_2\left(\frac{3}{4}\right) - \frac{1}{4}\log_2\left(\frac{1}{4}\right)\)

\(P(a_1 = T) = 4/9\)

\(H(a_1) = -\frac{5}{9}\left[\frac{4}{5}\log_2\left(\frac{4}{5}\right)+\frac{1}{5}\log_2\left(\frac{1}{5}\right)\right] -\frac{4}{9}\left[\frac{3}{4}\log_2\left(\frac{3}{4}\right) + \frac{1}{4}\log_2\left(\frac{1}{4}\right)\right] =\)
0.7616392

Subtract this from original entropy to get information gain:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a1_entropy <-}\StringTok{ }\DecValTok{-5}\OperatorTok{/}\DecValTok{9} \OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{4}\OperatorTok{/}\DecValTok{5}\OperatorTok{*}\KeywordTok{log2}\NormalTok{(}\DecValTok{4}\OperatorTok{/}\DecValTok{5}\NormalTok{) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\OperatorTok{/}\DecValTok{5}\OperatorTok{*}\KeywordTok{log2}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{5}\NormalTok{)) }\OperatorTok{+}\StringTok{ }\DecValTok{-4}\OperatorTok{/}\DecValTok{9} \OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{3}\OperatorTok{/}\DecValTok{4}\OperatorTok{*}\KeywordTok{log2}\NormalTok{(}\DecValTok{3}\OperatorTok{/}\DecValTok{4}\NormalTok{) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\OperatorTok{/}\DecValTok{4}\OperatorTok{*}\KeywordTok{log2}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{4}\NormalTok{))}
\NormalTok{entropy <-}\StringTok{ }\DecValTok{-4}\OperatorTok{/}\DecValTok{9} \OperatorTok{*}\StringTok{ }\KeywordTok{log2}\NormalTok{(}\DecValTok{4}\OperatorTok{/}\DecValTok{9}\NormalTok{) }\OperatorTok{-}\StringTok{ }\DecValTok{5}\OperatorTok{/}\DecValTok{9} \OperatorTok{*}\StringTok{ }\KeywordTok{log2}\NormalTok{(}\DecValTok{5}\OperatorTok{/}\DecValTok{9}\NormalTok{)}

\NormalTok{entropy }\OperatorTok{-}\StringTok{ }\NormalTok{a1_entropy}
\end{Highlighting}
\end{Shaded}

Follow the same process for \(a_2\):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ftable}\NormalTok{(Table3}\FloatTok{.6}\NormalTok{[,}\KeywordTok{c}\NormalTok{(}\StringTok{"a2"}\NormalTok{, }\StringTok{"TargetClass"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\(H(a_2 = F) = -\frac{1}{2}\log_2\left(\frac{1}{2}\right)-\frac{1}{2}\log_2\left(\frac{1}{2}\right)\)

\(P(a_2 = F) = 4/9\)

\(H(a_2 = T) = -\frac{3}{5}\log_2\left(\frac{3}{5}\right) - \frac{2}{5}\log_2\left(\frac{2}{5}\right)\)

\(P(a_2 = T) = 5/9\)

\(H(a_2) = -\frac{4}{9}\left[\frac{1}{2}\log_2\left(\frac{1}{2}\right)+\frac{1}{2}\log_2\left(\frac{1}{2}\right)\right] -\frac{5}{9}\left[\frac{3}{5}\log_2\left(\frac{3}{5}\right) + \frac{2}{5}\log_2\left(\frac{2}{5}\right)\right] =\)
0.9838614

Subtract from entropy to get an information gain of:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a2_entropy <-}\StringTok{ }\DecValTok{-4}\OperatorTok{/}\DecValTok{9} \OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{2}\OperatorTok{*}\KeywordTok{log2}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\OperatorTok{/}\DecValTok{2}\OperatorTok{*}\KeywordTok{log2}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{2}\NormalTok{)) }\OperatorTok{+}\StringTok{ }\DecValTok{-5}\OperatorTok{/}\DecValTok{9} \OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{3}\OperatorTok{/}\DecValTok{5}\OperatorTok{*}\KeywordTok{log2}\NormalTok{(}\DecValTok{3}\OperatorTok{/}\DecValTok{5}\NormalTok{) }\OperatorTok{+}\StringTok{ }\DecValTok{2}\OperatorTok{/}\DecValTok{5}\OperatorTok{*}\KeywordTok{log2}\NormalTok{(}\DecValTok{2}\OperatorTok{/}\DecValTok{5}\NormalTok{))}
\NormalTok{entropy }\OperatorTok{-}\StringTok{ }\NormalTok{a2_entropy}
\end{Highlighting}
\end{Shaded}

So we find much greater information gain from \(a_1\) than \(a_2\),
which suggests splitting and classifying the data on \(a_1\) is better.

\hypertarget{e}{%
\paragraph{e)}\label{e}}

What is the best split, between \(a_1\) and \(a_2\), according to the
misclassification error rate?

Misclassification error rate is simply the number of incorrect
classifications divided by \(N = 9\)

Revisiting the table for \(a_1\):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ftable}\NormalTok{(Table3}\FloatTok{.6}\NormalTok{[,}\KeywordTok{c}\NormalTok{(}\StringTok{"a1"}\NormalTok{, }\StringTok{"TargetClass"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

If our classification rule is - if \(a_1 = F\) and + if \(a_1 = T\), ,
then we would incorrectly classify one item as - because \(a_1 = F\),
and incorrectly classify one item as + with \(a_1 = T\). This leads to a
misclassification rate of \(2/9\)

Table for \(a_2\):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ftable}\NormalTok{(Table3}\FloatTok{.6}\NormalTok{[,}\KeywordTok{c}\NormalTok{(}\StringTok{"a2"}\NormalTok{, }\StringTok{"TargetClass"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

For classifying on \(a_2 = F\), it is split 2:2 on whether or not to
classify as a + or -. Either way, we have two misclassifications.
Classifying for \(a_2 = T\), we will classify as a -, with accuracy
\(3/5\), which gives us 2 misclassifications. The total
misclassification rate on \(a_2 = 4/9\)

Thus, using misclassification rate, the best split is again on \(a_1\)

\hypertarget{f-what-is-the-best-split-between-a_1-and-a_2-according-to-the-gini-index}{%
\paragraph{\texorpdfstring{f) What is the best split, between \(a_1\)
and \(a_2\), according to the Gini
index?}{f) What is the best split, between a\_1 and a\_2, according to the Gini index?}}\label{f-what-is-the-best-split-between-a_1-and-a_2-according-to-the-gini-index}}

\(Gini(N) = 1 - \sum [p(i|N)]^2\)

For \(a_1\):

\(Gini(a_1 = F )=1 −- P(-− | a_1 = F )^2 -− P(+ | a_1 = F )^2 = 1 − ( \frac{4}{5})^2 − (\frac{1}{5})^2\)

\(Gini(a_1 = T )= 1 -− P ( -− | a_1 = T )^2 −- P ( + | a_1 = T )^2 = 1 − ( \frac{1}{4} )^2 − ( \frac{3}{4} )^2\)

\end{document}
